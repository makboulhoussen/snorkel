{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude Snorkel / Extration de partenariats d'entreprises\n",
    "Pour ce projet, nous allons essayer d'extraire des relations de partenariats économiques entre des entreprises depuis des articles. <br/>\n",
    "Le but étant de comprendre l'utilisation de snorkel.\n",
    "\n",
    "Nous allons découper notre traitement dans 3 notebooks (chacun correspondant à une étape du pipeline `Snorkel` :\n",
    "1. Preprocessing\n",
    "2. Training\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie I: Corpus Preprocessing\n",
    "\n",
    "Dans ce notebook, nous allons pré-traiter plusieurs documents en utilisant la solution `Snorkel`.\n",
    "Nous allons commencer par \"parser\" les documents pour construire notre hiérarchie de _contexts_.\n",
    "Nous allons ensuite instancier nos _candidats_ qui correspondent aux objets que nous voulons classer. Dans notre cas identifier les relations de partenariat entre 2 entreprises.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from snorkel.models import StableLabel\n",
    "from snorkel.db_helpers import reload_annotator_labels\n",
    "from snorkel.parser import TSVDocPreprocessor\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import OrganizationMatcher, DictionaryMatch\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# Connect to the database backend and initalize a Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des noms d'entreprises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les entreprises à étudier ont été stockées dans un fichier `pickle` dans l'étape **préparation des données** (`prepare_data.py`)\n",
    "On commence par charger ces noms des entreprises utiles pour le parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'entreprises 868\n"
     ]
    }
   ],
   "source": [
    "with open('./data/companies.pkl', 'rb') as f:\n",
    "    company_names = pickle.load(f)\n",
    "print(\"Nombre d'entreprises\", len(company_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des découpages des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'étape **préparation des données** (`prepare_data.py`) nous avons aussi réparti nos données en 3 ensembles : entrainement, développement et tests. Chargons cette répartition stockée dans des fichiers `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set length : 368 documents\n",
      "dev set length   : 45 documents\n",
      "test set length  : 45 documents\n"
     ]
    }
   ],
   "source": [
    "with open('./data/train_set.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "with open('./data/dev_set.pkl', 'rb') as f:\n",
    "    dev_set = pickle.load(f)\n",
    "with open('./data/test_set.pkl', 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "print(\"train set length :\", len(train_set), 'documents')\n",
    "print(\"dev set length   :\", len(dev_set), 'documents')\n",
    "print(\"test set length  :\", len(test_set), 'documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Parsing de nos articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration d'un `DocPreprocessor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons charger le corpus de documents (dans notre cas les articles) et effectuer les pré-traitements sur les données. <br/>\n",
    "Nos articles  sont stockés sous la forme : nom du document et texte associé.<br/>\n",
    "Nous allons utiliser la classe `TSVDocPreprocessor` pour lire ces documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = 1000\n",
    "doc_preprocessor = TSVDocPreprocessor('data/articles.tsv', max_docs=n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation d'un `CorpusParser`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite nous appuyer sur la librairie [Spacy](https://spacy.io/), un parser NLP, afin de découper nos documents en phrases et tokens et appliquer un NER (Named-entity Recognition) : classer les entités nommés dans des catégories (ex : Personne, Organisation, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 7.02 s, sys: 931 ms, total: 7.95 s\n",
      "Wall time: 8.89 s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont stockées dans une base SQLLite par Snorkel. Nous pouvons faire quelques requêtes pour vérifier que nos données sont correctement enregistrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 458\n",
      "Sentences: 472\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Génération des candidats\n",
    "\n",
    "Dans cette partie nous allons extraire les _candidats__ de notre corpus. Ce sont les objets pour lesquels nous souhaitons faire une prédiction. \n",
    "Dans notre cas, un candidat est une paire de nom d'entreprise trouvée dans une phrase et pour lesquels nous voulons prédire s'ils sont partenaires ou non."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Définition du schéma de candidat\n",
    "\n",
    "Nous avons défini une relation binaire _competitor_ qui associe deux objets texte de type `Span`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Partner = candidate_subclass('Partner', ['company1', 'company2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction des candidats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant extraire nos candidats en identifiant dans chaque phrase, les paires de n-grams (jusqu'à 6) que l'on retrouve dans notre dictionnaire de noms d'entreprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams         = Ngrams(n_max=6)\n",
    "company_matcher = DictionaryMatch(d=company_names, ignore_case=True)\n",
    "cand_extractor = CandidateExtractor(Partner, [ngrams, ngrams], [company_matcher, company_matcher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite nous découpons nos données en jeu d'entrainement (80%), de developpements (10%) et de tests (10%) en nous basons sur le découpage pré-établie dans la phase préparation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if (doc.name in dev_set) :\n",
    "            dev_sents.add(s)\n",
    "        elif (doc.name in test_set) :\n",
    "            test_sents.add(s)      \n",
    "        else:\n",
    "            train_sents.add(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous appliquons ensuite notre extracteur de candidats sur les 3 jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Number of candidates: 2464\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Number of candidates: 166\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Number of candidates: 304\n",
      "CPU times: user 8.02 s, sys: 329 ms, total: 8.35 s\n",
      "Wall time: 8.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    cand_extractor.apply(sents, split=i)\n",
    "    print(\"Number of candidates:\", session.query(Partner).filter(Partner.split == i).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Chargement des données labelisées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous enregistrons maintenant les données qui ont été identifiés manuellement comme des partenaires. <br/>\n",
    "- Ces données sont appelés les gold-labels et ils nous serviront à évaluer nos algorithmes.<br/>\n",
    "- Nous mettons ensuite à jour les annotations pour les données de développement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATH = 'data/gold_labels.tsv'\n",
    "\n",
    "def load_external_labels(session, candidate_class, annotator_name='gold'):\n",
    "    gold_labels = pd.read_csv(FPATH, sep=\"\\t\")\n",
    "    for index, row in gold_labels.iterrows(): \n",
    "        # We check if the label already exists, in case this cell was already executed\n",
    "        context_stable_ids = \"~~\".join([row['company1'], row['company2']])\n",
    "    \n",
    "        query = session.query(StableLabel).filter(StableLabel.context_stable_ids == context_stable_ids)\n",
    "        query = query.filter(StableLabel.annotator_name == annotator_name)\n",
    "        if query.count() == 0:\n",
    "            session.add(StableLabel(\n",
    "                context_stable_ids=context_stable_ids,\n",
    "                annotator_name=annotator_name,\n",
    "                value=row['label']))\n",
    "                    \n",
    "        # Because it's a symmetric relation, load both directions...\n",
    "        context_stable_ids = \"~~\".join([row['company2'], row['company1']])\n",
    "        \n",
    "        query = session.query(StableLabel).filter(StableLabel.context_stable_ids == context_stable_ids)\n",
    "        query = query.filter(StableLabel.annotator_name == annotator_name)\n",
    "        if query.count() == 0:\n",
    "            session.add(StableLabel(\n",
    "                context_stable_ids=context_stable_ids,\n",
    "                annotator_name=annotator_name,\n",
    "                value=row['label']))\n",
    "    # Commit session\n",
    "    session.commit()\n",
    "    \n",
    "    # Reload annotator labels\n",
    "    reload_annotator_labels(session, candidate_class, annotator_name, split=1, filter_label_split=False)\n",
    "    reload_annotator_labels(session, candidate_class, annotator_name, split=2, filter_label_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 103\n",
      "AnnotatorLabels created: 181\n",
      "CPU times: user 47.2 s, sys: 907 ms, total: 48.1 s\n",
      "Wall time: 52.5 s\n"
     ]
    }
   ],
   "source": [
    "%time missed = load_external_labels(session, Partner, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Contrôle du chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous faisons quelques requêtes en base pour s'assurer que nos données sont correctement enregistrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = session.query(Partner).filter(Partner.split == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Atos']\n",
      "['VMware']\n"
     ]
    }
   ],
   "source": [
    "print (cands[0].company1.get_attrib_tokens(a='words'))\n",
    "print (cands[0].company2.get_attrib_tokens(a='words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sentence :  Atos has developed a multivendor alliancewith Dell EMC, Intel, Juniper Networks, Red Hat and VMware, aiming to accelerate network functions virtualization (NFV) deployments in service provider networks\n",
      "\n",
      "- Doc Id =  8eb02e8d-ff02-489f-8fa0-bb0036ef5008\n"
     ]
    }
   ],
   "source": [
    "sentence = cands[0].get_parent()\n",
    "print(\"- Sentence : \", sentence.text)\n",
    "document = sentence.get_parent()\n",
    "print(\"- Doc Id = \", document.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests des `Labeling Functions` helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,get_text_splits,\n",
    "    contains_token, rule_text_btw, rule_text_in_span\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_candidate = cands[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' has developed a multivendor alliancewith Dell EMC, Intel, Juniper Networks, Red Hat and '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_between(my_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has\n",
      "developed\n",
      "a\n",
      "multivendor\n",
      "alliancewith\n",
      "dell\n",
      "emc\n",
      ",\n",
      "intel\n",
      ",\n",
      "juniper\n",
      "networks\n",
      ",\n",
      "red\n",
      "hat\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "for t in get_between_tokens(my_candidate) :\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in get_left_tokens(my_candidate, window=5) :\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "aiming\n",
      "to\n",
      "accelerate\n",
      "network\n"
     ]
    }
   ],
   "source": [
    "for t in get_right_tokens(my_candidate, window=5) :\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{A}} has developed a multivendor alliancewith Dell EMC, Intel, Juniper Networks, Red Hat and {{B}}, aiming to accelerate network functions virtualization (NFV) deployments in service provider networks\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tagged_text(my_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '{{A}}',\n",
       " ' has developed a multivendor alliancewith Dell EMC, Intel, Juniper Networks, Red Hat and ',\n",
       " '{{B}}',\n",
       " ', aiming to accelerate network functions virtualization (NFV) deployments in service provider networks\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_splits(my_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
